import json

# Load the notebook
notebook_path = 'notebooks/02_data_cleaning.ipynb'
with open(notebook_path, 'r', encoding='utf-8') as f:
    notebook = json.load(f)

# Create the Q&A cell
qa_cell = {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
        "---\n",
        "## 10. Data Cleaning Q&A\n",
        "\n",
        "### Q1: Explain how you handled missing values\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "I handled missing values using a **systematic comparison approach** to identify the most appropriate imputation method for this dataset. Here's the detailed process:\n",
        "\n",
        "1. **Identification Phase:**\n",
        "   - First, I identified which columns had missing values using `df.isnull().sum()`\n",
        "   - Calculated the percentage of missing data for each column\n",
        "   - Found that columns like `reviews_per_month`, `last_review`, and `name` had missing values\n",
        "\n",
        "2. **Testing Multiple Methods:**\n",
        "   I tested four different imputation techniques on the dataset:\n",
        "   \n",
        "   - **Forward Fill (ffill):** Propagates the last valid observation forward\n",
        "   - **Backward Fill (bfill):** Uses the next valid observation to fill gaps\n",
        "   - **Interpolation:** Estimates missing values based on linear relationships between data points\n",
        "   - **Mean/Median Imputation:** Fills missing values with statistical measures (mean for normally distributed data, median for skewed data)\n",
        "\n",
        "3. **Comparison and Selection:**\n",
        "   - Created separate dataframes for each method to compare their effectiveness\n",
        "   - Visualized the remaining missing values after each method\n",
        "   - **Selected Mean/Median Imputation** as the best approach because:\n",
        "     - It completely eliminated all missing values\n",
        "     - It's statistically sound for this type of dataset\n",
        "     - It doesn't introduce temporal bias (unlike forward/backward fill)\n",
        "     - It's robust to outliers when using median\n",
        "\n",
        "4. **Implementation Details:**\n",
        "   - Used **median** for numerical columns (like `reviews_per_month`, `price`) to handle skewed distributions and outliers\n",
        "   - Used **mode** for categorical columns to preserve the most common category\n",
        "   - Converted `last_review` to datetime format and handled missing dates appropriately\n",
        "\n",
        "---\n",
        "\n",
        "### Q2: Why did you choose mean/median imputation?\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "I chose **mean/median imputation** (specifically **median imputation** for most numerical features) for several important reasons:\n",
        "\n",
        "**1. Complete Missing Value Resolution:**\n",
        "   - Mean/median imputation successfully filled **100% of missing values**, while other methods left some gaps\n",
        "   - This ensures the dataset is ready for machine learning algorithms that cannot handle missing data\n",
        "\n",
        "**2. Statistical Appropriateness:**\n",
        "   - **Median is robust to outliers:** In datasets like Airbnb listings, features like `price` and `reviews_per_month` often have extreme values (luxury listings, viral properties). The median is not affected by these outliers, unlike the mean.\n",
        "   - **Preserves central tendency:** Using median maintains the typical value for each feature without being skewed by extreme cases\n",
        "\n",
        "**3. Avoids Temporal Bias:**\n",
        "   - Unlike forward fill or backward fill, median imputation doesn't assume any temporal relationship in the data\n",
        "   - This is crucial for Airbnb data where listings are independent entities, not time-series observations\n",
        "\n",
        "**4. Simplicity and Interpretability:**\n",
        "   - The method is straightforward to understand and explain\n",
        "   - It's a well-established baseline approach in data science\n",
        "   - Easy to reproduce and validate\n",
        "\n",
        "**5. Domain-Specific Reasoning:**\n",
        "   - For `reviews_per_month`: Missing values likely indicate listings with no reviews. Using median (which represents typical review frequency) is a reasonable assumption.\n",
        "   - For `price`: Median price represents the typical market rate, avoiding distortion from luxury or budget outliers.\n",
        "\n",
        "**When I used Mean vs. Median:**\n",
        "   - **Median:** For skewed distributions (price, reviews_per_month, availability)\n",
        "   - **Mean:** For approximately normal distributions (if any existed)\n",
        "   - **Mode:** For categorical features (neighbourhood_group, room_type)\n",
        "\n",
        "---\n",
        "\n",
        "### Q3: What are the limitations of forward fill?\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "Forward fill (ffill) has several significant limitations that made it unsuitable for this dataset:\n",
        "\n",
        "**1. Assumes Temporal or Sequential Order:**\n",
        "   - Forward fill propagates the **last valid observation** forward\n",
        "   - This assumes the data has a meaningful sequential order (like time-series data)\n",
        "   - **Problem for Airbnb data:** Listings are independent entities with no inherent order. The row order in the CSV is arbitrary.\n",
        "   - Using ffill would mean a listing's missing price might be filled with the price of a completely unrelated listing that happened to appear earlier in the file.\n",
        "\n",
        "**2. Cannot Fill Leading Missing Values:**\n",
        "   - If the first rows in the dataset have missing values, forward fill **cannot fill them** (there's no previous value to propagate)\n",
        "   - This leaves gaps in the data, making it incomplete for analysis\n",
        "\n",
        "**3. Propagates Potentially Irrelevant Information:**\n",
        "   - Example: If listing #50 has a missing `price`, ffill would use the price from listing #49\n",
        "   - These two listings might be in different neighborhoods, different room types, and completely different price ranges\n",
        "   - This creates **artificial relationships** that don't exist in reality\n",
        "\n",
        "**4. Introduces Bias:**\n",
        "   - Forward fill can create **autocorrelation** where none exists\n",
        "   - It reduces variance in the data by duplicating values\n",
        "   - This can mislead machine learning models into finding patterns that aren't real\n",
        "\n",
        "**5. Not Statistically Sound for Cross-Sectional Data:**\n",
        "   - For cross-sectional data (snapshot of different entities at one point in time), forward fill has no theoretical justification\n",
        "   - It's essentially a random imputation based on arbitrary row ordering\n",
        "\n",
        "**6. Poor Performance in This Dataset:**\n",
        "   - As shown in the comparison visualization, forward fill **did not eliminate all missing values**\n",
        "   - It left gaps that would still need to be addressed\n",
        "\n",
        "**When Forward Fill IS Appropriate:**\n",
        "   - **Time-series data:** Stock prices, sensor readings, weather data where the last known value is a reasonable estimate\n",
        "   - **Sequential measurements:** When observations are naturally ordered and related\n",
        "   - **Carry-forward assumptions:** When it's reasonable to assume a value persists until a new measurement is taken\n",
        "\n",
        "**Example of the Problem:**\n",
        "```\n",
        "Row 100: Luxury Manhattan Apartment, price = $500, reviews_per_month = NaN\n",
        "Row 99: Budget Brooklyn Room, price = $50, reviews_per_month = 2.5\n",
        "\n",
        "Forward fill would set Row 100's reviews_per_month = 2.5\n",
        "This is meaningless because these listings are completely unrelated!\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### Q4: How can improper data cleaning affect ML models?\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "Improper data cleaning can have **severe and far-reaching consequences** on machine learning models. Here are the key impacts:\n",
        "\n",
        "**1. Biased Predictions:**\n",
        "   - **Example:** If we used forward fill inappropriately, we'd create artificial patterns in the data\n",
        "   - The model would learn these false patterns and make biased predictions\n",
        "   - **Real-world impact:** A price prediction model might systematically undervalue or overvalue certain types of listings\n",
        "\n",
        "**2. Reduced Model Accuracy:**\n",
        "   - Incorrect imputation introduces **noise** into the training data\n",
        "   - The model learns from incorrect values, leading to poor generalization\n",
        "   - **Metrics affected:** Lower R¬≤, higher RMSE, reduced classification accuracy\n",
        "   - **Example:** If we filled missing prices with mean instead of median in a highly skewed distribution, luxury outliers would inflate the mean, causing systematic errors\n",
        "\n",
        "**3. Overfitting or Underfitting:**\n",
        "   - **Overfitting:** If we create artificial patterns (like with inappropriate forward fill), the model might memorize these patterns instead of learning real relationships\n",
        "   - **Underfitting:** If we remove too many rows with missing values, we might lose important information and the model won't have enough data to learn\n",
        "\n",
        "**4. Data Leakage:**\n",
        "   - **Critical issue:** Using information from the test set to impute training set values\n",
        "   - **Example:** Calculating mean from the entire dataset (train + test) before splitting\n",
        "   - **Consequence:** Artificially inflated performance metrics that don't reflect real-world performance\n",
        "\n",
        "**5. Violation of Model Assumptions:**\n",
        "   - Many ML algorithms assume:\n",
        "     - Features are independent\n",
        "     - Data follows certain distributions\n",
        "     - No systematic bias in the data\n",
        "   - Improper cleaning can violate these assumptions\n",
        "   - **Example:** Forward fill creates autocorrelation, violating independence assumptions\n",
        "\n",
        "**6. Incorrect Feature Importance:**\n",
        "   - If a feature is improperly cleaned, the model might:\n",
        "     - Overestimate its importance (if artificial patterns are created)\n",
        "     - Underestimate its importance (if noise is introduced)\n",
        "   - This misleads feature selection and interpretation\n",
        "\n",
        "**7. Poor Generalization to New Data:**\n",
        "   - A model trained on improperly cleaned data won't perform well on real-world data\n",
        "   - **Example:** If we filled missing `reviews_per_month` with the mean (say, 3.5), but in reality, missing values represent listings with 0 reviews, the model will make poor predictions for new listings with no reviews\n",
        "\n",
        "**8. Amplification of Outliers or Suppression of Variance:**\n",
        "   - **Using mean instead of median:** Outliers disproportionately affect the mean, spreading their influence to all imputed values\n",
        "   - **Using mode excessively:** Can reduce variance and make the dataset less representative\n",
        "   - **Impact:** Models might become insensitive to important variations in the data\n",
        "\n",
        "**9. Ethical and Fairness Issues:**\n",
        "   - Improper imputation can introduce or amplify bias against certain groups\n",
        "   - **Example:** If missing values are more common in certain neighborhoods and we impute them incorrectly, we might create systematic bias in predictions for those areas\n",
        "   - This can lead to unfair or discriminatory model behavior\n",
        "\n",
        "**10. Wasted Resources:**\n",
        "   - Training models on poorly cleaned data wastes computational resources\n",
        "   - Debugging model performance issues caused by data quality is time-consuming\n",
        "   - May require complete retraining after discovering data issues\n",
        "\n",
        "**Specific Examples from This Dataset:**\n",
        "\n",
        "| Improper Cleaning | Consequence |\n",
        "|-------------------|-------------|\n",
        "| Using forward fill for `price` | Model learns false price relationships based on arbitrary row order |\n",
        "| Deleting all rows with missing `reviews_per_month` | Lose all listings with no reviews, creating selection bias |\n",
        "| Using mean for `price` instead of median | Luxury listings inflate imputed values, causing systematic overestimation |\n",
        "| Not converting `last_review` to datetime | Model can't use temporal features, losing predictive power |\n",
        "| Keeping duplicate rows | Model overweights certain listings, creating bias |\n",
        "\n",
        "**Best Practices to Avoid These Issues:**\n",
        "\n",
        "1. **Understand your data:** Know why values are missing (MCAR, MAR, or MNAR)\n",
        "2. **Choose appropriate methods:** Match imputation strategy to data type and distribution\n",
        "3. **Document decisions:** Keep track of all cleaning steps for reproducibility\n",
        "4. **Validate results:** Check distributions before and after cleaning\n",
        "5. **Avoid data leakage:** Compute imputation statistics only on training data\n",
        "6. **Consider domain knowledge:** Use subject matter expertise to guide decisions\n",
        "7. **Test sensitivity:** Try multiple approaches and compare model performance\n",
        "\n",
        "**In this notebook, I avoided these issues by:**\n",
        "- Testing multiple imputation methods systematically\n",
        "- Choosing median over mean to handle outliers\n",
        "- Avoiding temporal methods (ffill/bfill) for cross-sectional data\n",
        "- Verifying complete missing value resolution\n",
        "- Documenting all assumptions and decisions\n",
        "\n",
        "---\n",
        "\n"
    ]
}

# Find the index of the summary section (currently section 10)
summary_index = None
for i, cell in enumerate(notebook['cells']):
    if cell['cell_type'] == 'markdown' and any('## 10. Summary of Data Cleaning Process' in line for line in cell['source']):
        summary_index = i
        break

if summary_index is not None:
    # Insert the Q&A section before the summary
    notebook['cells'].insert(summary_index, qa_cell)
    
    # Update the summary section number from 10 to 11
    for line_idx, line in enumerate(notebook['cells'][summary_index + 1]['source']):
        if '## 10. Summary of Data Cleaning Process' in line:
            notebook['cells'][summary_index + 1]['source'][line_idx] = line.replace('## 10. Summary', '## 11. Summary')
            break
    
    # Save the updated notebook
    with open(notebook_path, 'w', encoding='utf-8') as f:
        json.dump(notebook, f, indent=4, ensure_ascii=False)
    
    print("‚úÖ Q&A section successfully added to 02_data_cleaning.ipynb")
    print(f"   - Inserted Q&A as section 10")
    print(f"   - Updated Summary section to section 11")
    print("\nüìù Next steps:")
    print("   1. Open the notebook in Jupyter to review the changes")
    print("   2. Commit and push to GitHub")
else:
    print("‚ùå Could not find the Summary section in the notebook")
