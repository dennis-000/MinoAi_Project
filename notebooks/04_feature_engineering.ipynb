{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Notebook 04: Feature Engineering\n",
                "\n",
                "## Purpose\n",
                "This notebook prepares features for machine learning by:\n",
                "- Encoding categorical variables\n",
                "- Scaling numerical features\n",
                "- Creating new derived features\n",
                "- Selecting relevant features\n",
                "\n",
                "## Learning Objectives\n",
                "- Apply feature transformation techniques\n",
                "- Understand when to use different encoding methods\n",
                "- Create meaningful derived features\n",
                "- Prepare data for modeling"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 1. Import Libraries and Load Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder\n",
                "from sklearn.model_selection import train_test_split\n",
                "\n",
                "# Display settings\n",
                "pd.set_option('display.max_columns', None)\n",
                "plt.style.use('seaborn-v0_8-whitegrid')\n",
                "\n",
                "print(\"Libraries imported successfully!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load cleaned dataset\n",
                "df = pd.read_csv('../data/cleaned_dataset.csv')\n",
                "\n",
                "print(f\"Dataset loaded: {df.shape[0]:,} rows, {df.shape[1]} columns\")\n",
                "df.head()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 2. Feature Selection\n",
                "\n",
                "Select features that will be useful for predicting price."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Define features for modeling\n",
                "# We'll exclude ID columns, names, and the target variable\n",
                "\n",
                "# Target variable\n",
                "target = 'price'\n",
                "\n",
                "# Features to exclude\n",
                "exclude_features = ['id', 'name', 'host_id', 'host_name', 'last_review', 'price']\n",
                "\n",
                "# Select features\n",
                "feature_columns = [col for col in df.columns if col not in exclude_features]\n",
                "\n",
                "print(\"SELECTED FEATURES:\")\n",
                "print(\"=\"*80)\n",
                "for i, col in enumerate(feature_columns, 1):\n",
                "    print(f\"{i:2d}. {col}\")\n",
                "\n",
                "print(f\"\\nTotal features: {len(feature_columns)}\")\n",
                "print(f\"Target variable: {target}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 3. Create Derived Features\n",
                "\n",
                "### Assumption:\n",
                "Creating new features from existing ones can improve model performance by capturing additional patterns."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create a copy for feature engineering\n",
                "df_fe = df.copy()\n",
                "\n",
                "# Feature 1: Reviews per availability (popularity metric)\n",
                "df_fe['reviews_per_availability'] = df_fe['number_of_reviews'] / (df_fe['availability_365'] + 1)\n",
                "\n",
                "# Feature 2: Is the listing highly available? (binary)\n",
                "df_fe['high_availability'] = (df_fe['availability_365'] > 180).astype(int)\n",
                "\n",
                "# Feature 3: Has reviews (binary)\n",
                "df_fe['has_reviews'] = (df_fe['number_of_reviews'] > 0).astype(int)\n",
                "\n",
                "# Feature 4: Price category (based on quartiles)\n",
                "# This won't be used as a feature but helps understand price segments\n",
                "price_quartiles = df_fe['price'].quantile([0.25, 0.5, 0.75])\n",
                "\n",
                "print(\"NEW DERIVED FEATURES CREATED:\")\n",
                "print(\"=\"*80)\n",
                "print(\"1. reviews_per_availability - Reviews normalized by availability\")\n",
                "print(\"2. high_availability - Binary indicator for highly available listings\")\n",
                "print(\"3. has_reviews - Binary indicator for listings with reviews\")\n",
                "\n",
                "print(\"\\nSample of new features:\")\n",
                "df_fe[['reviews_per_availability', 'high_availability', 'has_reviews']].head(10)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 4. Encode Categorical Variables\n",
                "\n",
                "### 4.1 One-Hot Encoding for Room Type\n",
                "\n",
                "**Explanation**: One-hot encoding creates binary columns for each category. This is suitable for nominal categorical variables where there's no inherent order."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# One-hot encode room_type\n",
                "if 'room_type' in df_fe.columns:\n",
                "    room_type_encoded = pd.get_dummies(df_fe['room_type'], prefix='room_type', drop_first=True)\n",
                "    df_fe = pd.concat([df_fe, room_type_encoded], axis=1)\n",
                "    \n",
                "    print(\"ONE-HOT ENCODED COLUMNS (room_type):\")\n",
                "    print(\"=\"*80)\n",
                "    print(room_type_encoded.columns.tolist())\n",
                "    print(\"\\nSample:\")\n",
                "    print(room_type_encoded.head())"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 4.2 Label Encoding for Neighbourhood Group\n",
                "\n",
                "**Explanation**: Label encoding assigns a unique integer to each category. While this can introduce ordinal relationships, it's memory-efficient for high-cardinality features."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Label encode neighbourhood_group\n",
                "if 'neighbourhood_group' in df_fe.columns:\n",
                "    le = LabelEncoder()\n",
                "    df_fe['neighbourhood_group_encoded'] = le.fit_transform(df_fe['neighbourhood_group'])\n",
                "    \n",
                "    # Create mapping for reference\n",
                "    mapping = dict(zip(le.classes_, le.transform(le.classes_)))\n",
                "    \n",
                "    print(\"LABEL ENCODING MAPPING (neighbourhood_group):\")\n",
                "    print(\"=\"*80)\n",
                "    for category, code in mapping.items():\n",
                "        print(f\"{category}: {code}\")\n",
                "    \n",
                "    print(\"\\nSample:\")\n",
                "    print(df_fe[['neighbourhood_group', 'neighbourhood_group_encoded']].head(10))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 4.3 Handle Neighbourhood (High Cardinality)\n",
                "\n",
                "**Explanation**: The neighbourhood column has many unique values. We'll use frequency encoding or group rare categories."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Frequency encoding for neighbourhood\n",
                "if 'neighbourhood' in df_fe.columns:\n",
                "    neighbourhood_freq = df_fe['neighbourhood'].value_counts(normalize=True)\n",
                "    df_fe['neighbourhood_frequency'] = df_fe['neighbourhood'].map(neighbourhood_freq)\n",
                "    \n",
                "    print(f\"Neighbourhood unique values: {df_fe['neighbourhood'].nunique()}\")\n",
                "    print(\"\\nTop 10 neighbourhoods by frequency:\")\n",
                "    print(neighbourhood_freq.head(10))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 5. Feature Scaling\n",
                "\n",
                "### 5.1 StandardScaler (Z-score normalization)\n",
                "\n",
                "**Explanation**: StandardScaler transforms features to have mean=0 and std=1. This is suitable for features with normal distribution."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Select numerical features for scaling\n",
                "numerical_features = ['latitude', 'longitude', 'minimum_nights', 'number_of_reviews', \n",
                "                      'reviews_per_month', 'calculated_host_listings_count', 'availability_365',\n",
                "                      'reviews_per_availability', 'neighbourhood_frequency']\n",
                "\n",
                "# Filter to only include columns that exist\n",
                "numerical_features = [col for col in numerical_features if col in df_fe.columns]\n",
                "\n",
                "# Apply StandardScaler\n",
                "scaler_standard = StandardScaler()\n",
                "df_fe_scaled_standard = df_fe.copy()\n",
                "df_fe_scaled_standard[numerical_features] = scaler_standard.fit_transform(df_fe[numerical_features])\n",
                "\n",
                "print(\"STANDARD SCALER APPLIED\")\n",
                "print(\"=\"*80)\n",
                "print(\"Features scaled:\")\n",
                "for col in numerical_features:\n",
                "    print(f\"  - {col}\")\n",
                "\n",
                "print(\"\\nScaled features statistics:\")\n",
                "print(df_fe_scaled_standard[numerical_features].describe())"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 5.2 MinMaxScaler (0-1 normalization)\n",
                "\n",
                "**Explanation**: MinMaxScaler transforms features to a [0, 1] range. This is useful when you want to preserve the shape of the distribution."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Apply MinMaxScaler\n",
                "scaler_minmax = MinMaxScaler()\n",
                "df_fe_scaled_minmax = df_fe.copy()\n",
                "df_fe_scaled_minmax[numerical_features] = scaler_minmax.fit_transform(df_fe[numerical_features])\n",
                "\n",
                "print(\"MINMAX SCALER APPLIED\")\n",
                "print(\"=\"*80)\n",
                "print(\"\\nScaled features statistics:\")\n",
                "print(df_fe_scaled_minmax[numerical_features].describe())"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 5.3 Compare Scaling Methods"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize scaling comparison\n",
                "sample_feature = numerical_features[0] if numerical_features else 'latitude'\n",
                "\n",
                "if sample_feature in df_fe.columns:\n",
                "    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
                "    \n",
                "    # Original\n",
                "    axes[0].hist(df_fe[sample_feature], bins=50, color='skyblue', edgecolor='black')\n",
                "    axes[0].set_title(f'Original: {sample_feature}', fontsize=11, fontweight='bold')\n",
                "    axes[0].set_xlabel(sample_feature)\n",
                "    axes[0].set_ylabel('Frequency')\n",
                "    \n",
                "    # StandardScaler\n",
                "    axes[1].hist(df_fe_scaled_standard[sample_feature], bins=50, color='lightgreen', edgecolor='black')\n",
                "    axes[1].set_title(f'StandardScaler: {sample_feature}', fontsize=11, fontweight='bold')\n",
                "    axes[1].set_xlabel(f'{sample_feature} (scaled)')\n",
                "    axes[1].set_ylabel('Frequency')\n",
                "    \n",
                "    # MinMaxScaler\n",
                "    axes[2].hist(df_fe_scaled_minmax[sample_feature], bins=50, color='coral', edgecolor='black')\n",
                "    axes[2].set_title(f'MinMaxScaler: {sample_feature}', fontsize=11, fontweight='bold')\n",
                "    axes[2].set_xlabel(f'{sample_feature} (scaled)')\n",
                "    axes[2].set_ylabel('Frequency')\n",
                "    \n",
                "    plt.tight_layout()\n",
                "    plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 6. Final Feature Set Preparation\n",
                "\n",
                "### Decision: We'll use StandardScaler for our final model\n",
                "\n",
                "**Rationale**: StandardScaler is more robust to outliers and works well with most ML algorithms."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Prepare final feature set\n",
                "df_final = df_fe_scaled_standard.copy()\n",
                "\n",
                "# Select final features for modeling\n",
                "final_features = [\n",
                "    # Scaled numerical features\n",
                "    'latitude', 'longitude', 'minimum_nights', 'number_of_reviews',\n",
                "    'reviews_per_month', 'calculated_host_listings_count', 'availability_365',\n",
                "    # Derived features\n",
                "    'reviews_per_availability', 'high_availability', 'has_reviews',\n",
                "    # Encoded categorical features\n",
                "    'neighbourhood_group_encoded', 'neighbourhood_frequency'\n",
                "]\n",
                "\n",
                "# Add one-hot encoded room_type columns\n",
                "room_type_cols = [col for col in df_final.columns if col.startswith('room_type_')]\n",
                "final_features.extend(room_type_cols)\n",
                "\n",
                "# Filter to only include columns that exist\n",
                "final_features = [col for col in final_features if col in df_final.columns]\n",
                "\n",
                "print(\"FINAL FEATURE SET:\")\n",
                "print(\"=\"*80)\n",
                "for i, col in enumerate(final_features, 1):\n",
                "    print(f\"{i:2d}. {col}\")\n",
                "\n",
                "print(f\"\\nTotal features for modeling: {len(final_features)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 7. Prepare X and y for Modeling"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Prepare features (X) and target (y)\n",
                "X = df_final[final_features]\n",
                "y = df_final['price']\n",
                "\n",
                "print(\"MODELING DATA PREPARED:\")\n",
                "print(\"=\"*80)\n",
                "print(f\"Feature matrix (X) shape: {X.shape}\")\n",
                "print(f\"Target vector (y) shape: {y.shape}\")\n",
                "print(f\"\\nFeature matrix sample:\")\n",
                "print(X.head())\n",
                "print(f\"\\nTarget variable sample:\")\n",
                "print(y.head())"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 8. Train-Test Split\n",
                "\n",
                "**Assumption**: We use 80-20 split with random_state for reproducibility."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Split data into training and testing sets\n",
                "X_train, X_test, y_train, y_test = train_test_split(\n",
                "    X, y, test_size=0.2, random_state=42\n",
                ")\n",
                "\n",
                "print(\"TRAIN-TEST SPLIT:\")\n",
                "print(\"=\"*80)\n",
                "print(f\"Training set size: {X_train.shape[0]:,} samples ({X_train.shape[0]/len(X)*100:.1f}%)\")\n",
                "print(f\"Testing set size: {X_test.shape[0]:,} samples ({X_test.shape[0]/len(X)*100:.1f}%)\")\n",
                "print(f\"\\nFeatures: {X_train.shape[1]}\")\n",
                "print(f\"\\nTraining target statistics:\")\n",
                "print(y_train.describe())\n",
                "print(f\"\\nTesting target statistics:\")\n",
                "print(y_test.describe())"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 9. Save Engineered Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save engineered features\n",
                "X_train.to_csv('../data/X_train.csv', index=False)\n",
                "X_test.to_csv('../data/X_test.csv', index=False)\n",
                "y_train.to_csv('../data/y_train.csv', index=False, header=['price'])\n",
                "y_test.to_csv('../data/y_test.csv', index=False, header=['price'])\n",
                "\n",
                "print(\"Engineered data saved:\")\n",
                "print(\"  - ../data/X_train.csv\")\n",
                "print(\"  - ../data/X_test.csv\")\n",
                "print(\"  - ../data/y_train.csv\")\n",
                "print(\"  - ../data/y_test.csv\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 10. Summary\n",
                "\n",
                "### Feature Engineering Completed:\n",
                "\n",
                "1. **Derived Features** ✅\n",
                "   - Created `reviews_per_availability`\n",
                "   - Created `high_availability` binary indicator\n",
                "   - Created `has_reviews` binary indicator\n",
                "\n",
                "2. **Categorical Encoding** ✅\n",
                "   - One-hot encoded `room_type`\n",
                "   - Label encoded `neighbourhood_group`\n",
                "   - Frequency encoded `neighbourhood`\n",
                "\n",
                "3. **Feature Scaling** ✅\n",
                "   - Applied StandardScaler to numerical features\n",
                "   - Compared with MinMaxScaler\n",
                "   - Selected StandardScaler for final model\n",
                "\n",
                "4. **Data Preparation** ✅\n",
                "   - Created feature matrix (X) and target vector (y)\n",
                "   - Split into training (80%) and testing (20%) sets\n",
                "   - Saved engineered data for modeling\n",
                "\n",
                "### Key Assumptions:\n",
                "\n",
                "- StandardScaler is appropriate for our features\n",
                "- 80-20 train-test split provides sufficient data for both training and evaluation\n",
                "- One-hot encoding for room_type won't cause dimensionality issues (only 3 categories)\n",
                "- Frequency encoding captures neighbourhood importance\n",
                "\n",
                "### Next Steps:\n",
                "\n",
                "The engineered features are now ready for **Machine Learning Modeling** in the next notebook, where we will:\n",
                "- Train multiple regression models\n",
                "- Compare model performance\n",
                "- Generate predictions\n",
                "\n",
                "---\n",
                "**Next Notebook**: [05_modeling.ipynb](05_modeling.ipynb)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}