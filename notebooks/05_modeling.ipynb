{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Notebook 05: Machine Learning Modeling\n",
                "\n",
                "## Purpose\n",
                "This notebook applies machine learning algorithms to predict Airbnb listing prices:\n",
                "- Define the prediction problem\n",
                "- Train multiple regression models\n",
                "- Compare model performance\n",
                "- Generate predictions\n",
                "\n",
                "## Learning Objectives\n",
                "- Formulate a machine learning problem\n",
                "- Apply different ML algorithms\n",
                "- Justify model selection\n",
                "- Document modeling process"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 1. Import Libraries and Load Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
                "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
                "from sklearn.tree import DecisionTreeRegressor\n",
                "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
                "import pickle\n",
                "\n",
                "# Display settings\n",
                "pd.set_option('display.max_columns', None)\n",
                "plt.style.use('seaborn-v0_8-whitegrid')\n",
                "\n",
                "print(\"Libraries imported successfully!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load engineered data\n",
                "X_train = pd.read_csv('../data/X_train.csv')\n",
                "X_test = pd.read_csv('../data/X_test.csv')\n",
                "y_train = pd.read_csv('../data/y_train.csv')['price']\n",
                "y_test = pd.read_csv('../data/y_test.csv')['price']\n",
                "\n",
                "print(\"DATA LOADED:\")\n",
                "print(\"=\"*80)\n",
                "print(f\"Training set: {X_train.shape[0]:,} samples, {X_train.shape[1]} features\")\n",
                "print(f\"Testing set: {X_test.shape[0]:,} samples, {X_test.shape[1]} features\")\n",
                "print(f\"\\nTarget variable range:\")\n",
                "print(f\"  Training: ${y_train.min():.2f} - ${y_train.max():.2f}\")\n",
                "print(f\"  Testing: ${y_test.min():.2f} - ${y_test.max():.2f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 2. Problem Formulation\n",
                "\n",
                "### Problem Type: Regression\n",
                "\n",
                "**Objective**: Predict the price of Airbnb listings based on their characteristics.\n",
                "\n",
                "**Why Regression?**\n",
                "- The target variable (price) is continuous\n",
                "- We want to predict actual dollar values, not categories\n",
                "- Regression models can capture the relationship between features and price\n",
                "\n",
                "**Features Used**:\n",
                "- Location (latitude, longitude, neighbourhood)\n",
                "- Property type (room type)\n",
                "- Availability metrics\n",
                "- Review statistics\n",
                "- Host information\n",
                "\n",
                "**Evaluation Metrics**:\n",
                "- Mean Absolute Error (MAE): Average prediction error in dollars\n",
                "- Root Mean Squared Error (RMSE): Penalizes larger errors more\n",
                "- R² Score: Proportion of variance explained by the model"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 3. Model Selection and Training\n",
                "\n",
                "We'll train multiple models and compare their performance:\n",
                "1. **Linear Regression** - Baseline model\n",
                "2. **Ridge Regression** - Linear with L2 regularization\n",
                "3. **Lasso Regression** - Linear with L1 regularization\n",
                "4. **Decision Tree** - Non-linear model\n",
                "5. **Random Forest** - Ensemble of decision trees\n",
                "6. **Gradient Boosting** - Advanced ensemble method"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 3.1 Linear Regression (Baseline)\n",
                "\n",
                "**Explanation**: Linear regression assumes a linear relationship between features and target. It's simple and interpretable."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Train Linear Regression\n",
                "print(\"Training Linear Regression...\")\n",
                "lr_model = LinearRegression()\n",
                "lr_model.fit(X_train, y_train)\n",
                "\n",
                "# Make predictions\n",
                "y_pred_lr_train = lr_model.predict(X_train)\n",
                "y_pred_lr_test = lr_model.predict(X_test)\n",
                "\n",
                "print(\"✓ Linear Regression trained\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 3.2 Ridge Regression\n",
                "\n",
                "**Explanation**: Ridge adds L2 regularization to prevent overfitting by penalizing large coefficients."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Train Ridge Regression\n",
                "print(\"Training Ridge Regression...\")\n",
                "ridge_model = Ridge(alpha=1.0, random_state=42)\n",
                "ridge_model.fit(X_train, y_train)\n",
                "\n",
                "# Make predictions\n",
                "y_pred_ridge_train = ridge_model.predict(X_train)\n",
                "y_pred_ridge_test = ridge_model.predict(X_test)\n",
                "\n",
                "print(\"✓ Ridge Regression trained\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 3.3 Lasso Regression\n",
                "\n",
                "**Explanation**: Lasso adds L1 regularization which can perform feature selection by setting some coefficients to zero."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Train Lasso Regression\n",
                "print(\"Training Lasso Regression...\")\n",
                "lasso_model = Lasso(alpha=1.0, random_state=42)\n",
                "lasso_model.fit(X_train, y_train)\n",
                "\n",
                "# Make predictions\n",
                "y_pred_lasso_train = lasso_model.predict(X_train)\n",
                "y_pred_lasso_test = lasso_model.predict(X_test)\n",
                "\n",
                "print(\"✓ Lasso Regression trained\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 3.4 Decision Tree Regressor\n",
                "\n",
                "**Explanation**: Decision trees can capture non-linear relationships and interactions between features."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Train Decision Tree\n",
                "print(\"Training Decision Tree Regressor...\")\n",
                "dt_model = DecisionTreeRegressor(max_depth=10, random_state=42)\n",
                "dt_model.fit(X_train, y_train)\n",
                "\n",
                "# Make predictions\n",
                "y_pred_dt_train = dt_model.predict(X_train)\n",
                "y_pred_dt_test = dt_model.predict(X_test)\n",
                "\n",
                "print(\"✓ Decision Tree Regressor trained\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 3.5 Random Forest Regressor\n",
                "\n",
                "**Explanation**: Random Forest combines multiple decision trees to reduce overfitting and improve accuracy."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Train Random Forest\n",
                "print(\"Training Random Forest Regressor...\")\n",
                "rf_model = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42, n_jobs=-1)\n",
                "rf_model.fit(X_train, y_train)\n",
                "\n",
                "# Make predictions\n",
                "y_pred_rf_train = rf_model.predict(X_train)\n",
                "y_pred_rf_test = rf_model.predict(X_test)\n",
                "\n",
                "print(\"✓ Random Forest Regressor trained\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 3.6 Gradient Boosting Regressor\n",
                "\n",
                "**Explanation**: Gradient Boosting builds trees sequentially, each correcting errors from previous trees."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Train Gradient Boosting\n",
                "print(\"Training Gradient Boosting Regressor...\")\n",
                "gb_model = GradientBoostingRegressor(n_estimators=100, max_depth=5, random_state=42)\n",
                "gb_model.fit(X_train, y_train)\n",
                "\n",
                "# Make predictions\n",
                "y_pred_gb_train = gb_model.predict(X_train)\n",
                "y_pred_gb_test = gb_model.predict(X_test)\n",
                "\n",
                "print(\"✓ Gradient Boosting Regressor trained\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 4. Model Evaluation\n",
                "\n",
                "### 4.1 Calculate Metrics for All Models"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Function to calculate metrics\n",
                "def calculate_metrics(y_true, y_pred, dataset_name, model_name):\n",
                "    mae = mean_absolute_error(y_true, y_pred)\n",
                "    mse = mean_squared_error(y_true, y_pred)\n",
                "    rmse = np.sqrt(mse)\n",
                "    r2 = r2_score(y_true, y_pred)\n",
                "    \n",
                "    return {\n",
                "        'Model': model_name,\n",
                "        'Dataset': dataset_name,\n",
                "        'MAE': mae,\n",
                "        'RMSE': rmse,\n",
                "        'R2_Score': r2\n",
                "    }\n",
                "\n",
                "# Calculate metrics for all models\n",
                "results = []\n",
                "\n",
                "models = [\n",
                "    ('Linear Regression', y_pred_lr_train, y_pred_lr_test),\n",
                "    ('Ridge Regression', y_pred_ridge_train, y_pred_ridge_test),\n",
                "    ('Lasso Regression', y_pred_lasso_train, y_pred_lasso_test),\n",
                "    ('Decision Tree', y_pred_dt_train, y_pred_dt_test),\n",
                "    ('Random Forest', y_pred_rf_train, y_pred_rf_test),\n",
                "    ('Gradient Boosting', y_pred_gb_train, y_pred_gb_test)\n",
                "]\n",
                "\n",
                "for model_name, y_pred_train, y_pred_test in models:\n",
                "    results.append(calculate_metrics(y_train, y_pred_train, 'Training', model_name))\n",
                "    results.append(calculate_metrics(y_test, y_pred_test, 'Testing', model_name))\n",
                "\n",
                "# Create results dataframe\n",
                "results_df = pd.DataFrame(results)\n",
                "\n",
                "print(\"MODEL EVALUATION RESULTS:\")\n",
                "print(\"=\"*80)\n",
                "print(results_df.to_string(index=False))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 4.2 Visualize Model Comparison"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Separate training and testing results\n",
                "test_results = results_df[results_df['Dataset'] == 'Testing'].copy()\n",
                "\n",
                "# Create comparison plots\n",
                "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
                "\n",
                "# MAE comparison\n",
                "axes[0].barh(test_results['Model'], test_results['MAE'], color='coral', edgecolor='black')\n",
                "axes[0].set_xlabel('Mean Absolute Error ($)', fontsize=11)\n",
                "axes[0].set_title('MAE Comparison (Lower is Better)', fontsize=12, fontweight='bold')\n",
                "axes[0].grid(axis='x', alpha=0.3)\n",
                "\n",
                "# RMSE comparison\n",
                "axes[1].barh(test_results['Model'], test_results['RMSE'], color='skyblue', edgecolor='black')\n",
                "axes[1].set_xlabel('Root Mean Squared Error ($)', fontsize=11)\n",
                "axes[1].set_title('RMSE Comparison (Lower is Better)', fontsize=12, fontweight='bold')\n",
                "axes[1].grid(axis='x', alpha=0.3)\n",
                "\n",
                "# R² comparison\n",
                "axes[2].barh(test_results['Model'], test_results['R2_Score'], color='lightgreen', edgecolor='black')\n",
                "axes[2].set_xlabel('R² Score', fontsize=11)\n",
                "axes[2].set_title('R² Score Comparison (Higher is Better)', fontsize=12, fontweight='bold')\n",
                "axes[2].grid(axis='x', alpha=0.3)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 4.3 Identify Best Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Find best model based on test R² score\n",
                "best_model_row = test_results.loc[test_results['R2_Score'].idxmax()]\n",
                "\n",
                "print(\"BEST MODEL:\")\n",
                "print(\"=\"*80)\n",
                "print(f\"Model: {best_model_row['Model']}\")\n",
                "print(f\"MAE: ${best_model_row['MAE']:.2f}\")\n",
                "print(f\"RMSE: ${best_model_row['RMSE']:.2f}\")\n",
                "print(f\"R² Score: {best_model_row['R2_Score']:.4f}\")\n",
                "print(f\"\\nThis model explains {best_model_row['R2_Score']*100:.2f}% of the variance in price.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 5. Feature Importance (Random Forest)\n",
                "\n",
                "Understanding which features are most important for prediction."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Get feature importance from Random Forest\n",
                "feature_importance = pd.DataFrame({\n",
                "    'Feature': X_train.columns,\n",
                "    'Importance': rf_model.feature_importances_\n",
                "}).sort_values('Importance', ascending=False)\n",
                "\n",
                "# Plot top 15 features\n",
                "plt.figure(figsize=(10, 8))\n",
                "plt.barh(feature_importance.head(15)['Feature'], \n",
                "         feature_importance.head(15)['Importance'], \n",
                "         color='teal', edgecolor='black')\n",
                "plt.xlabel('Importance', fontsize=11)\n",
                "plt.ylabel('Feature', fontsize=11)\n",
                "plt.title('Top 15 Most Important Features (Random Forest)', fontsize=13, fontweight='bold')\n",
                "plt.gca().invert_yaxis()\n",
                "plt.grid(axis='x', alpha=0.3)\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "print(\"\\nTOP 10 MOST IMPORTANT FEATURES:\")\n",
                "print(\"=\"*80)\n",
                "print(feature_importance.head(10).to_string(index=False))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 6. Save Models\n",
                "\n",
                "Save trained models for use in the evaluation notebook."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save all models\n",
                "models_to_save = {\n",
                "    'linear_regression': lr_model,\n",
                "    'ridge_regression': ridge_model,\n",
                "    'lasso_regression': lasso_model,\n",
                "    'decision_tree': dt_model,\n",
                "    'random_forest': rf_model,\n",
                "    'gradient_boosting': gb_model\n",
                "}\n",
                "\n",
                "for model_name, model in models_to_save.items():\n",
                "    with open(f'../data/{model_name}_model.pkl', 'wb') as f:\n",
                "        pickle.dump(model, f)\n",
                "    print(f\"✓ Saved {model_name}\")\n",
                "\n",
                "# Save predictions\n",
                "predictions_df = pd.DataFrame({\n",
                "    'actual': y_test,\n",
                "    'lr_pred': y_pred_lr_test,\n",
                "    'ridge_pred': y_pred_ridge_test,\n",
                "    'lasso_pred': y_pred_lasso_test,\n",
                "    'dt_pred': y_pred_dt_test,\n",
                "    'rf_pred': y_pred_rf_test,\n",
                "    'gb_pred': y_pred_gb_test\n",
                "})\n",
                "\n",
                "predictions_df.to_csv('../data/predictions.csv', index=False)\n",
                "print(\"\\n✓ Saved predictions\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 7. Summary\n",
                "\n",
                "### Models Trained:\n",
                "\n",
                "1. **Linear Regression** ✅ - Simple baseline\n",
                "2. **Ridge Regression** ✅ - L2 regularization\n",
                "3. **Lasso Regression** ✅ - L1 regularization with feature selection\n",
                "4. **Decision Tree** ✅ - Non-linear relationships\n",
                "5. **Random Forest** ✅ - Ensemble method\n",
                "6. **Gradient Boosting** ✅ - Advanced ensemble\n",
                "\n",
                "### Key Findings:\n",
                "\n",
                "- Tree-based models (Random Forest, Gradient Boosting) generally perform better than linear models\n",
                "- Location features (latitude, longitude) are among the most important predictors\n",
                "- Room type and neighbourhood significantly impact price\n",
                "- The best model achieves reasonable predictive accuracy\n",
                "\n",
                "### Model Selection Justification:\n",
                "\n",
                "We chose to train multiple models because:\n",
                "- Different algorithms have different strengths\n",
                "- Comparison helps identify the best approach for this data\n",
                "- Ensemble methods often outperform single models\n",
                "- Linear models provide interpretability while tree-based models capture complexity\n",
                "\n",
                "### Next Steps:\n",
                "\n",
                "In the next notebook, we will:\n",
                "- Perform detailed evaluation of all models\n",
                "- Visualize predictions vs actual values\n",
                "- Analyze prediction errors\n",
                "- Discuss limitations and improvements\n",
                "- Draw final conclusions\n",
                "\n",
                "---\n",
                "**Next Notebook**: [06_evaluation_and_insights.ipynb](06_evaluation_and_insights.ipynb)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}