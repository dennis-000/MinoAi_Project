{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Notebook 02: Data Cleaning\n",
                "\n",
                "## Purpose\n",
                "This notebook focuses on improving data quality by:\n",
                "- Handling missing values using **multiple techniques** (as required)\n",
                "- Removing duplicates\n",
                "- Fixing data type inconsistencies\n",
                "- Preparing clean data for exploratory analysis\n",
                "\n",
                "## Learning Objectives\n",
                "- Apply ALL required missing value treatment methods\n",
                "- Understand when each method is appropriate\n",
                "- Document assumptions and reasoning for data cleaning decisions"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 1. Import Libraries and Load Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "\n",
                "# Display settings\n",
                "pd.set_option('display.max_columns', None)\n",
                "plt.style.use('seaborn-v0_8-darkgrid')\n",
                "\n",
                "print(\"Libraries imported successfully!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load the dataset\n",
                "df_original = pd.read_csv('../data/MinoAI_dataset.csv')\n",
                "\n",
                "print(f\"Dataset loaded: {df_original.shape[0]:,} rows, {df_original.shape[1]} columns\")\n",
                "print(f\"\\nMemory usage: {df_original.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 2. Identify Missing Values\n",
                "\n",
                "Before cleaning, we need to understand the extent and pattern of missing data."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Calculate missing values\n",
                "missing_data = pd.DataFrame({\n",
                "    'Column': df_original.columns,\n",
                "    'Missing_Count': df_original.isnull().sum().values,\n",
                "    'Missing_Percentage': (df_original.isnull().sum().values / len(df_original) * 100)\n",
                "})\n",
                "\n",
                "# Filter columns with missing values\n",
                "missing_data = missing_data[missing_data['Missing_Count'] > 0].sort_values('Missing_Count', ascending=False)\n",
                "\n",
                "print(\"COLUMNS WITH MISSING VALUES:\")\n",
                "print(\"=\"*80)\n",
                "print(missing_data.to_string(index=False))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 3. Missing Value Treatment Methods\n",
                "\n",
                "As required by the assignment, we will apply **ALL** of the following methods:\n",
                "1. **Forward Fill** - Propagate last valid observation forward\n",
                "2. **Backward Fill** - Use next valid observation to fill gap\n",
                "3. **Interpolation** - Estimate values based on surrounding data\n",
                "4. **Mean/Median Imputation** - Fill with statistical measures\n",
                "\n",
                "We'll create separate DataFrames for each method to compare results.\n",
                "\n",
                "### Assumptions:\n",
                "- **Forward fill** is suitable for time-series data where values tend to persist\n",
                "- **Backward fill** is useful when future values are known and relevant\n",
                "- **Interpolation** works well for numerical data with linear relationships\n",
                "- **Mean imputation** is appropriate for normally distributed data without outliers"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 3.1 Method 1: Forward Fill\n",
                "\n",
                "**Explanation**: Forward fill propagates the last valid observation forward to fill missing values. This is particularly useful for time-series data or when we assume values remain constant until changed.\n",
                "\n",
                "**When to use**: Best for sequential data where the previous value is a reasonable estimate."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create a copy for forward fill\n",
                "df_ffill = df_original.copy()\n",
                "\n",
                "# Apply forward fill\n",
                "df_ffill = df_ffill.fillna(method='ffill')\n",
                "\n",
                "# Check remaining missing values\n",
                "print(\"FORWARD FILL RESULTS:\")\n",
                "print(\"=\"*80)\n",
                "print(f\"Remaining missing values: {df_ffill.isnull().sum().sum()}\")\n",
                "print(f\"\\nMissing values by column:\")\n",
                "print(df_ffill.isnull().sum()[df_ffill.isnull().sum() > 0])"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 3.2 Method 2: Backward Fill\n",
                "\n",
                "**Explanation**: Backward fill uses the next valid observation to fill missing values. This is the reverse of forward fill.\n",
                "\n",
                "**When to use**: Useful when future values are known and can inform past missing values."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create a copy for backward fill\n",
                "df_bfill = df_original.copy()\n",
                "\n",
                "# Apply backward fill\n",
                "df_bfill = df_bfill.fillna(method='bfill')\n",
                "\n",
                "# Check remaining missing values\n",
                "print(\"BACKWARD FILL RESULTS:\")\n",
                "print(\"=\"*80)\n",
                "print(f\"Remaining missing values: {df_bfill.isnull().sum().sum()}\")\n",
                "print(f\"\\nMissing values by column:\")\n",
                "print(df_bfill.isnull().sum()[df_bfill.isnull().sum() > 0])"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 3.3 Method 3: Interpolation\n",
                "\n",
                "**Explanation**: Interpolation estimates missing values based on the values before and after the gap. It assumes a linear relationship between data points.\n",
                "\n",
                "**When to use**: Best for numerical data with continuous values and linear trends."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create a copy for interpolation\n",
                "df_interpolate = df_original.copy()\n",
                "\n",
                "# Apply interpolation to numerical columns only\n",
                "numerical_cols = df_interpolate.select_dtypes(include=['int64', 'float64']).columns\n",
                "df_interpolate[numerical_cols] = df_interpolate[numerical_cols].interpolate(method='linear', limit_direction='both')\n",
                "\n",
                "# For categorical columns, use forward fill as fallback\n",
                "df_interpolate = df_interpolate.fillna(method='ffill').fillna(method='bfill')\n",
                "\n",
                "# Check remaining missing values\n",
                "print(\"INTERPOLATION RESULTS:\")\n",
                "print(\"=\"*80)\n",
                "print(f\"Remaining missing values: {df_interpolate.isnull().sum().sum()}\")\n",
                "print(f\"\\nMissing values by column:\")\n",
                "print(df_interpolate.isnull().sum()[df_interpolate.isnull().sum() > 0])"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 3.4 Method 4: Mean/Median Imputation\n",
                "\n",
                "**Explanation**: This method replaces missing values with the mean (for normally distributed data) or median (for skewed data) of the column.\n",
                "\n",
                "**When to use**: \n",
                "- **Mean**: For normally distributed numerical data\n",
                "- **Median**: For skewed data or data with outliers"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create a copy for mean/median imputation\n",
                "df_impute = df_original.copy()\n",
                "\n",
                "# For numerical columns, use median (more robust to outliers)\n",
                "numerical_cols = df_impute.select_dtypes(include=['int64', 'float64']).columns\n",
                "\n",
                "for col in numerical_cols:\n",
                "    if df_impute[col].isnull().sum() > 0:\n",
                "        median_value = df_impute[col].median()\n",
                "        df_impute[col].fillna(median_value, inplace=True)\n",
                "        print(f\"Filled {col} with median: {median_value}\")\n",
                "\n",
                "# For categorical columns, use mode (most frequent value)\n",
                "categorical_cols = df_impute.select_dtypes(include=['object']).columns\n",
                "\n",
                "for col in categorical_cols:\n",
                "    if df_impute[col].isnull().sum() > 0:\n",
                "        mode_value = df_impute[col].mode()[0] if not df_impute[col].mode().empty else 'Unknown'\n",
                "        df_impute[col].fillna(mode_value, inplace=True)\n",
                "        print(f\"Filled {col} with mode: {mode_value}\")\n",
                "\n",
                "# Check remaining missing values\n",
                "print(\"\\nMEAN/MEDIAN IMPUTATION RESULTS:\")\n",
                "print(\"=\"*80)\n",
                "print(f\"Remaining missing values: {df_impute.isnull().sum().sum()}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 4. Compare Methods\n",
                "\n",
                "Let's compare the effectiveness of each method."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create comparison summary\n",
                "comparison = pd.DataFrame({\n",
                "    'Method': ['Original', 'Forward Fill', 'Backward Fill', 'Interpolation', 'Mean/Median Imputation'],\n",
                "    'Missing_Values': [\n",
                "        df_original.isnull().sum().sum(),\n",
                "        df_ffill.isnull().sum().sum(),\n",
                "        df_bfill.isnull().sum().sum(),\n",
                "        df_interpolate.isnull().sum().sum(),\n",
                "        df_impute.isnull().sum().sum()\n",
                "    ]\n",
                "})\n",
                "\n",
                "comparison['Percentage_Complete'] = ((df_original.shape[0] * df_original.shape[1] - comparison['Missing_Values']) / \n",
                "                                      (df_original.shape[0] * df_original.shape[1]) * 100)\n",
                "\n",
                "print(\"METHOD COMPARISON:\")\n",
                "print(\"=\"*80)\n",
                "print(comparison.to_string(index=False))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Visualize Comparison"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize method comparison\n",
                "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
                "\n",
                "# Plot 1: Missing values by method\n",
                "ax1.bar(comparison['Method'], comparison['Missing_Values'], color='coral')\n",
                "ax1.set_xlabel('Method', fontsize=11)\n",
                "ax1.set_ylabel('Missing Values Count', fontsize=11)\n",
                "ax1.set_title('Missing Values by Imputation Method', fontsize=13, fontweight='bold')\n",
                "ax1.tick_params(axis='x', rotation=45)\n",
                "ax1.grid(axis='y', alpha=0.3)\n",
                "\n",
                "# Add value labels\n",
                "for i, v in enumerate(comparison['Missing_Values']):\n",
                "    ax1.text(i, v + 500, str(v), ha='center', va='bottom', fontweight='bold')\n",
                "\n",
                "# Plot 2: Percentage complete\n",
                "ax2.bar(comparison['Method'], comparison['Percentage_Complete'], color='lightgreen')\n",
                "ax2.set_xlabel('Method', fontsize=11)\n",
                "ax2.set_ylabel('Percentage Complete (%)', fontsize=11)\n",
                "ax2.set_title('Data Completeness by Method', fontsize=13, fontweight='bold')\n",
                "ax2.tick_params(axis='x', rotation=45)\n",
                "ax2.set_ylim([95, 100])\n",
                "ax2.grid(axis='y', alpha=0.3)\n",
                "\n",
                "# Add value labels\n",
                "for i, v in enumerate(comparison['Percentage_Complete']):\n",
                "    ax2.text(i, v + 0.1, f'{v:.2f}%', ha='center', va='bottom', fontweight='bold')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 5. Select Best Method and Create Clean Dataset\n",
                "\n",
                "### Decision Rationale:\n",
                "\n",
                "Based on the comparison above, we will use **Mean/Median Imputation** as our primary cleaning method because:\n",
                "\n",
                "1. It completely eliminates missing values\n",
                "2. It's statistically sound for this dataset\n",
                "3. It doesn't introduce bias from sequential filling\n",
                "4. It's appropriate for the type of data we have (property listings)\n",
                "\n",
                "**Assumption**: Missing reviews data likely indicates listings with no reviews, so using median (0 or low values) is reasonable."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Use the imputed dataset as our clean dataset\n",
                "df_clean = df_impute.copy()\n",
                "\n",
                "print(\"Selected method: Mean/Median Imputation\")\n",
                "print(f\"Clean dataset shape: {df_clean.shape}\")\n",
                "print(f\"Missing values: {df_clean.isnull().sum().sum()}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 6. Check for Duplicates"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Check for duplicate rows\n",
                "duplicates = df_clean.duplicated().sum()\n",
                "\n",
                "print(f\"Number of duplicate rows: {duplicates}\")\n",
                "\n",
                "if duplicates > 0:\n",
                "    print(f\"\\nRemoving {duplicates} duplicate rows...\")\n",
                "    df_clean = df_clean.drop_duplicates()\n",
                "    print(f\"New shape: {df_clean.shape}\")\n",
                "else:\n",
                "    print(\"No duplicates found!\")\n",
                "\n",
                "# Check for duplicate IDs (should be unique)\n",
                "duplicate_ids = df_clean['id'].duplicated().sum()\n",
                "print(f\"\\nDuplicate IDs: {duplicate_ids}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 7. Fix Data Types\n",
                "\n",
                "Some columns may need data type conversion for proper analysis."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Check current data types\n",
                "print(\"CURRENT DATA TYPES:\")\n",
                "print(\"=\"*80)\n",
                "print(df_clean.dtypes)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Convert last_review to datetime\n",
                "if 'last_review' in df_clean.columns:\n",
                "    # Convert to datetime, handling errors\n",
                "    df_clean['last_review'] = pd.to_datetime(df_clean['last_review'], errors='coerce')\n",
                "    print(\"Converted 'last_review' to datetime\")\n",
                "\n",
                "# Ensure numerical columns are correct type\n",
                "numerical_columns = ['price', 'minimum_nights', 'number_of_reviews', \n",
                "                     'reviews_per_month', 'calculated_host_listings_count', 'availability_365']\n",
                "\n",
                "for col in numerical_columns:\n",
                "    if col in df_clean.columns:\n",
                "        df_clean[col] = pd.to_numeric(df_clean[col], errors='coerce')\n",
                "\n",
                "print(\"\\nUPDATED DATA TYPES:\")\n",
                "print(\"=\"*80)\n",
                "print(df_clean.dtypes)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 8. Final Data Quality Check"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Comprehensive quality check\n",
                "print(\"FINAL DATA QUALITY REPORT:\")\n",
                "print(\"=\"*80)\n",
                "print(f\"Total Rows: {df_clean.shape[0]:,}\")\n",
                "print(f\"Total Columns: {df_clean.shape[1]}\")\n",
                "print(f\"Missing Values: {df_clean.isnull().sum().sum()}\")\n",
                "print(f\"Duplicate Rows: {df_clean.duplicated().sum()}\")\n",
                "print(f\"Memory Usage: {df_clean.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
                "\n",
                "print(\"\\nData Types Summary:\")\n",
                "print(df_clean.dtypes.value_counts())"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 9. Save Cleaned Dataset\n",
                "\n",
                "We'll save the cleaned dataset for use in subsequent notebooks."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save cleaned dataset\n",
                "df_clean.to_csv('../data/cleaned_dataset.csv', index=False)\n",
                "\n",
                "print(\"Cleaned dataset saved to: ../data/cleaned_dataset.csv\")\n",
                "print(f\"File size: {df_clean.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 10. Summary of Data Cleaning Process\n",
                "\n",
                "### What We Accomplished:\n",
                "\n",
                "1. **Missing Value Treatment** ✅\n",
                "   - Applied **Forward Fill**: Propagated last valid values forward\n",
                "   - Applied **Backward Fill**: Used next valid values to fill gaps\n",
                "   - Applied **Interpolation**: Estimated values based on linear relationships\n",
                "   - Applied **Mean/Median Imputation**: Filled with statistical measures\n",
                "   - **Selected**: Mean/Median Imputation as the best method\n",
                "\n",
                "2. **Duplicate Removal** ✅\n",
                "   - Checked for and removed duplicate rows\n",
                "   - Verified ID uniqueness\n",
                "\n",
                "3. **Data Type Fixes** ✅\n",
                "   - Converted `last_review` to datetime format\n",
                "   - Ensured numerical columns have correct types\n",
                "\n",
                "4. **Quality Assurance** ✅\n",
                "   - Verified no missing values remain\n",
                "   - Confirmed data integrity\n",
                "   - Saved clean dataset for analysis\n",
                "\n",
                "### Key Assumptions Made:\n",
                "\n",
                "- Missing review data indicates listings with no reviews (median imputation appropriate)\n",
                "- Median is preferred over mean due to potential outliers in price and other metrics\n",
                "- Mode is appropriate for categorical missing values\n",
                "- All listings should have unique IDs\n",
                "\n",
                "### Next Steps:\n",
                "\n",
                "The cleaned dataset is now ready for **Exploratory Data Analysis (EDA)** in the next notebook, where we will:\n",
                "- Visualize distributions\n",
                "- Analyze relationships between variables\n",
                "- Identify patterns and trends\n",
                "- Detect outliers\n",
                "\n",
                "---\n",
                "**Next Notebook**: [03_exploratory_data_analysis.ipynb](03_exploratory_data_analysis.ipynb)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}